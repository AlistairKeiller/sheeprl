# @package _global_

defaults:
  - override /algo: ppo_tsp
  - override /env: tsp
  - override /model_manager: ppo
  - _self_

# Algorithm
algo:
  total_steps: 1_280_000
  update_epochs: 2
  per_rank_batch_size: 512
  rollout_steps: 160
  mlp_keys:
    encoder: [nodes]
  other_keys:
    [edges, edge_links, mask, first_node, current_node]
  encoder:
    mlp_features_dim: 128
    num_layers: 3
  actor:
    num_heads: 8
  critic:
    num_heads: 8
  optimizer:
    lr: 1e-4

# Buffer
buffer:
  share_data: False
  size: ${algo.rollout_steps}

metric:
  aggregator:
    metrics:
      Loss/value_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/entropy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}

torch_deterministic: True