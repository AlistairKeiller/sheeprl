# @package _global_

defaults:
  - override /algo: sac
  - override /env: dmc
  - override /model_manager: sac
  - _self_

# Algorithm
algo:
  total_steps: 500000
  per_rank_batch_size: 256
  per_rank_gradient_steps: 16
  reset_frequency: 2560000
  mlp_keys:
    encoder: [state]
    decoder: [state]
  actor:
    optimizer:
      lr: 3e-4
      eps: 1.5e-4
  critic:
    optimizer:
      lr: 3e-4
      eps: 1.5e-4
  alpha:
    optimizer:
      lr: 3e-4
      eps: 1.5e-4

# Checkpoint
checkpoint:
  every: 50000

# Buffer
buffer:
  size: 1000000
  checkpoint: False
  sample_next_obs: False

# Environment
env:
  id: walker_run
  max_episode_steps: -1
  wrapper:
    from_pixels: False
    from_vectors: True

metric:
  aggregator:
    metrics:
      Loss/value_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/alpha_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
