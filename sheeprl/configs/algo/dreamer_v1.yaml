defaults:
  - default
  - /optim@world_model.optimizer: adam
  - /optim@actor.optimizer: adam
  - /optim@critic.optimizer: adam
  - _self_

gamma: 0.99
lmbda: 0.95
horizon: 15

# Model related parameters
dense_units: 400
mlp_layers: 4
dense_act: torch.nn.ELU
cnn_act: torch.nn.ReLU

world_model:
  stochastic_size: 30
  kl_free_nats: 3.0
  kl_free_avg: True
  kl_regularizer: 1.0
  continue_scale_factor: 1.0
  clip_gradients: 100.0

  encoder:
    cnn_channels_multiplier: 32
    cnn_act: ${algo.cnn_act}
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    dense_units: ${algo.dense_units}

  recurrent_model:
    recurrent_state_size: 200
    dense_units: ${algo.dense_units}

  transition_model:
    hidden_size: 200
    dense_act: ${algo.dense_act}

  representation_model:
    hidden_size: 200
    dense_act: ${algo.dense_act}

  observation_model:
    cnn_act: ${algo.cnn_act}
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    dense_units: ${algo.dense_units}

  reward_model:
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    dense_units: ${algo.dense_units}

  discount_model:
    learnable: True
    dense_act: ${algo.dense_act}
    mlp_layers: ${algo.mlp_layers}
    dense_units: ${algo.dense_units}

  optimizer:
    lr: 6e-4
    eps: 1e-5
    weight_decay: 0

actor:
  min_std: 0.1
  init_std: 5.0
  distribution: "auto"
  dense_act: ${algo.dense_act}
  mlp_layers: ${algo.mlp_layers}
  dense_units: ${algo.dense_units}
  clip_gradients: 100.0

  optimizer:
    lr: 8e-5
    eps: 1e-5
    weight_decay: 0

critic:
  dense_act: ${algo.dense_act}
  mlp_layers: ${algo.mlp_layers}
  dense_units: ${algo.dense_units}
  clip_gradients: 100.0

  optimizer:
    lr: 8e-5
    eps: 1e-5
    weight_decay: 0

player:
  expl_min: 0.0
  expl_amount: 0.3
  expl_decay: False
  max_step_expl_decay: 200000
